# ğŸ—ï¸ RAG Architecture Concepts - Complete Guide

**Document:** Retrieval-Augmented Generation (RAG) System Architecture  
**Project:** Capstone - Enterprise RAG Assistant  
**Date:** December 14, 2025  
**Version:** 1.0

---

## Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [Document Loaders](#1-document-loaders)
3. [Text Splitter](#2-text-splitter)
4. [Embedding](#3-embedding)
5. [Vector Store](#4-vector-store)
6. [Retriever](#5-retriever)
7. [Chains & Agents](#6-chains--agents)
8. [Memory](#7-memory)
9. [Complete RAG Flow](#complete-rag-flow)
10. [Configuration Reference](#configuration-reference)

---

## Architecture Overview

Your project is a **Retrieval-Augmented Generation (RAG)** system that combines document retrieval with AI language models to provide accurate, grounded responses.

### System Flow:

```
User Query
    â†“
[Document Loaders] â†’ Load files (PDF, DOCX, TXT, Images)
    â†“
[Text Splitter] â†’ Break into manageable chunks
    â†“
[Embeddings] â†’ Convert text to vectors
    â†“
[Vector Store] â†’ Store vectors in database
    â†“
[Retriever] â†’ Find relevant chunks for query
    â†“
[Chains] â†’ Combine context + query + LLM
    â†“
[Memory] â†’ Remember conversation history
    â†“
AI Response (grounded in your documents)
```

---

## 1. ğŸ“„ Document Loaders

### What It Is

Document Loaders are components that read files in various formats and convert them into a standardized format (Document objects) that can be processed by the rest of the pipeline.

### In Your Project

**Location:** `src/document_loader.py`

**Primary Function:**
```python
def load_documents(uploaded_files, llm_model):
    """
    Load documents from various file types.
    
    Args:
        uploaded_files: List of uploaded file objects
        llm_model: LLM model to use for image descriptions
    
    Returns:
        List of Document objects with page_content and metadata
    """
```

### Supported Formats

The system supports multiple file types:

- **PDF Files** - Extracts text from all pages
- **DOCX/DOC** - Reads Microsoft Word documents
- **TXT/CSV** - Plain text and comma-separated values
- **Images (PNG, JPG, JPEG, TIFF)** - Uses Vision AI to generate descriptions
- **ZIP Archives** - Automatically extracts and processes all files inside

### How It Works

**Process:**
1. User uploads file via Streamlit UI
2. Document Loader determines file type by extension
3. Appropriate parser extracts text/content
4. Metadata is collected (source filename, page number, etc.)
5. Returns standardized Document objects

**Example Processing:**

```
Input: PDF File "report.pdf" (20 pages)
    â†“
[PDF Reader extracts all pages]
    â†“
Output: Document {
    page_content: "Page 1 text content here...",
    metadata: {
        "source": "report.pdf",
        "page": 1
    }
}
```

### In Your Code (main.py)

```python
# From Ingest Tab
uploaded_files = st.file_uploader(
    "Upload files (PDF, DOCX, TXT, images)",
    type=["pdf", "docx", "png", "jpg", "jpeg", "tiff", "txt", "md", "doc"],
    accept_multiple_files=True
)

if st.button("ğŸ“¥ Ingest Uploaded Files"):
    docs_count, chunks_count = process_uploaded_files(
        uploaded_files,
        selected_model,
        llm_backend,
        db_path
    )
```

---

## 2. âœ‚ï¸ Text Splitter

### What It Is

Text Splitter breaks large documents into smaller, manageable pieces called "chunks" while preserving semantic meaning and context.

### In Your Project

**Location:** `src/embeddings.py`

**Implementation:**
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # Each chunk â‰ˆ 500 characters
    chunk_overlap=50,      # Overlap between chunks for context
    separators=["\n\n", "\n", " ", ""]
)

chunks = splitter.split_text(document_content)
```

### Why This Matters

| Problem | Solution |
|---------|----------|
| Large documents cause slow embedding processing | Split into smaller chunks (500 chars each) |
| LLM has token limits (4K-128K tokens) | Pass only relevant chunks to LLM |
| Loss of context at chunk boundaries | Use overlap (50 chars) between chunks |
| Memory constraints in vector database | Smaller chunks are faster to search |

### Visual Example

```
Original Document:
"Machine learning is a subset of artificial intelligence. Deep learning uses 
neural networks. Natural language processing analyzes text. Computer vision 
processes images..."

After Splitting (chunk_size=500, overlap=50):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk 1: "Machine learning is a subset of         â”‚
â”‚ artificial intelligence. Deep learning uses        â”‚
â”‚ neural networks. Natural language processing..."  â”‚  â† 500 chars
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (50 char overlap for context preservation)
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Chunk 2: "neural networks. Natural language       â”‚
     â”‚ processing analyzes text. Computer vision         â”‚
     â”‚ processes images..."                              â”‚  â† 500 chars
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configurable Parameters in Your UI

```python
chunk_size = st.number_input("Chunk Size", value=500, min_value=100)
chunk_overlap = st.number_input("Chunk Overlap", value=50, min_value=0)

st.session_state.chunk_size = chunk_size
st.session_state.chunk_overlap = chunk_overlap
```

### Best Practices

- **Chunk Size 200-500:** Better for semantic search
- **Chunk Size 1000+:** Better for summarization
- **Overlap 50-100:** Maintains context across boundaries
- **Default (500, 50):** Good balance for most use cases

---

## 3. ğŸ§  Embedding

### What It Is

Embeddings convert text into numerical vectors (arrays of floating-point numbers) that capture the semantic meaning of the text. Similar texts produce similar vectors.

### In Your Project

**Location:** `src/embeddings.py`

**Function:**
```python
def generate_embeddings(documents, embedding_model="text-embedding-3-small"):
    """Convert documents to embeddings using OpenAI API."""
    
    embeddings = OpenAIEmbeddings(model=embedding_model)
    
    # Convert text to vector
    vector = embeddings.embed_query("What is artificial intelligence?")
    # Returns: [0.123, -0.456, 0.789, ..., 0.234]  â† 384 or 1536 dimensions
```

### How It Works

**Process:**
1. Text input: "Paris is the capital of France"
2. Embedding model tokenizes and encodes the text
3. Returns vector representation: `[-0.042, 0.156, -0.289, ..., 0.501]`
4. Vector dimensions: 384 (small) or 1536 (large)

**Key Concept:**
- Semantically similar texts â†’ Numerically close vectors
- Different topics â†’ Distant vectors

**Example:**
```
"Paris is the capital of France"           â†’ Vector: [0.12, -0.34, 0.56, ...]
"France's capital is Paris"                â†’ Vector: [0.13, -0.35, 0.57, ...]
    â†‘ Very similar texts have similar vectors

"Bananas are yellow fruits"                â†’ Vector: [0.89, 0.12, -0.45, ...]
    â†‘ Different topic, very different vector
```

### Available Models

| Model | Dimensions | Speed | Accuracy | Cost | Best For |
|-------|-----------|-------|----------|------|----------|
| text-embedding-3-small | 384 | âš¡ Very Fast | Good | Lower | Large document sets, real-time |
| text-embedding-3-large | 1536 | ğŸ¢ Slower | Excellent | Higher | Precision-critical applications |

### Your Configuration

```python
# From Sidebar Configuration
embedding_model = st.selectbox(
    "Select Embedding Model",
    ["text-embedding-3-small", "text-embedding-3-large"],
    help="Choose the embedding model to use"
)
```

### Technical Details

- **Dimensionality:** Number of values in the vector
  - Small: 384 dimensions (lighter weight)
  - Large: 1536 dimensions (more expressive)

- **Tokenization:** Text split into tokens (~4 chars per token)
- **Encoding:** LLM transforms tokens to meaningful representations
- **Normalization:** Vectors normalized to unit length for comparison

---

## 4. ğŸ—„ï¸ Vector Store (ChromaDB)

### What It Is

Vector Store is a specialized database that stores embeddings and enables fast similarity searches. It maintains both the vectors and associated metadata.

### In Your Project

**Location:** `src/chroma_store.py`

**Implementation:**
```python
from langchain_community.vectorstores import Chroma

def store_in_vector_db(chunks, persist_directory, embedding_model):
    """Store chunks in vector database."""
    
    embeddings = OpenAIEmbeddings(model=embedding_model)
    
    vector_store = Chroma(
        persist_directory=persist_directory,  # ./data/chroma_db
        embedding_function=embeddings
    )
    
    # Add texts with embeddings
    vector_store.add_texts(
        texts=[chunk['page_content']],
        metadatas=[chunk['metadata']],
        ids=[chunk['id']],
        embeddings=[chunk['embedding']]  # Pre-computed
    )
    
    vector_store.persist()  # Save to disk
```

### Data Structure

Each stored item contains:

```json
{
    "id": "chunk_42",
    "text": "Machine learning is a subset of artificial intelligence...",
    "embedding": [0.123, -0.456, 0.789, ..., 0.234],
    "metadata": {
        "source": "AI_Guide.pdf",
        "page": 5,
        "chunk_index": 2
    }
}
```

### Directory Structure

Your ChromaDB storage location:

```
./data/chroma_db/
â”œâ”€â”€ .embedding_config.json      # Which embedding model was used
â”œâ”€â”€ chroma.sqlite3              # Main vector database file
â”œâ”€â”€ 5c66e64dd20c4234/           # Internal Chroma directory
â””â”€â”€ ...
```

### Why ChromaDB?

**Advantages:**
- âœ… **Persistent Storage:** Data saved to disk, survives app restart
- âœ… **Fast Retrieval:** Optimized for similarity searches
- âœ… **Metadata Support:** Store and filter by metadata
- âœ… **Open Source:** No licensing costs
- âœ… **Lightweight:** Runs locally without external servers
- âœ… **LangChain Integration:** Works seamlessly with your RAG pipeline

### Storage Statistics

In your Info Tab, you can see:

```
Vector Store Status:
â”œâ”€ Status: âœ… Active / âŒ Empty
â”œâ”€ Chunks: 150 (number of stored chunks)
â”œâ”€ Collections: 1
â”œâ”€ Model Used: text-embedding-3-small
â””â”€ Database Path: ./data/chroma_db
```

---

## 5. ğŸ” Retriever

### What It Is

Retriever performs similarity search to find the most relevant chunks from the vector store based on a user's query.

### In Your Project

**Location:** `src/retriever.py`

**Function:**
```python
def retrieve(query, k=3, db_path="./data/chroma_db", embedding_model="text-embedding-3-small"):
    """
    Find top-k most relevant chunks for a query.
    
    Args:
        query: User's question/search query
        k: Number of chunks to retrieve (default: 3)
        db_path: Path to vector database
        embedding_model: Embedding model to use for query
    
    Returns:
        List of Document objects (relevant chunks)
    """
```

### How It Works

**Step-by-Step Process:**

1. **Convert Query to Vector:**
   - User asks: "What is machine learning?"
   - Query embedding: `[-0.042, 0.156, -0.289, ...]`

2. **Calculate Similarity:**
   - Compare query vector with all stored chunk vectors
   - Use cosine similarity: values range from -1 to 1 (higher = more similar)

3. **Rank and Return:**
   - Chunk 1: "ML is a branch of AI" â†’ Similarity: 0.92 âœ…
   - Chunk 2: "Python programming language" â†’ Similarity: 0.12 âŒ
   - Chunk 3: "Deep learning uses neural networks" â†’ Similarity: 0.87 âœ…
   - Return top-k: [Chunk 1, Chunk 3, Chunk 5]

### Visual Example

```
User Query: "What is machine learning?"
    â†“
[Convert to Vector]
Vector: [-0.042, 0.156, -0.289, 0.501, ...]
    â†“
[Calculate Cosine Similarity]
Stored Vector 1: [0.100, 0.200, -0.150, 0.520, ...] â†’ Score: 0.92
Stored Vector 2: [0.850, -0.420, 0.680, -0.123, ...] â†’ Score: 0.12
Stored Vector 3: [0.050, 0.180, -0.310, 0.480, ...] â†’ Score: 0.87
    â†“
[Sort by Score]
    â†“
[Return Top-3]
1. Chunk with score 0.92
2. Chunk with score 0.87
3. Chunk with score 0.85
```

### Your Configuration

```python
# From Sidebar
n_retrieve = st.slider("Documents to Retrieve", 1, 10, 3)

# Used when creating RAG chain
top_chunks = retrieve(
    query=question,
    k=n_retrieve,  # 3 by default
    db_path=db_path
)
```

### Similarity Metrics

**Cosine Similarity:**
- Range: -1 to 1
- 1.0 = identical
- 0.9+ = very similar
- 0.5 = somewhat related
- 0.0+ = unrelated

---

## 6. ğŸ”— Chains & Agents

### What It Is

Chains orchestrate the RAG pipeline by combining retrieval, prompt formatting, and LLM invocation into a coherent flow.

### In Your Project

**Location:** `src/app_core.py`

**Main Function:**
```python
def create_simple_rag_chain(
    db_path, question, llm_model, temperature, n_retrieve=3, max_tokens=256
):
    """
    Create a RAG chain that:
    1. Retrieves relevant chunks
    2. Formats prompt with context
    3. Calls LLM
    4. Returns grounded response
    """
```

### Chain Flow

```
User Question: "What is prompt engineering?"
    â†“
[RETRIEVE STAGE]
    Fetch top-3 chunks from vector store
    â†“
[FORMAT STAGE]
    System Prompt: "You are a helpful assistant..."
    Context: "Prompt engineering is... [3 relevant chunks]..."
    Question: "What is prompt engineering?"
    â†“
[INVOKE STAGE]
    LLM Model: gpt-4o
    Temperature: 0.2 (grounded responses)
    Max Tokens: 256
    â†“
[RESPONSE STAGE]
    Return: "Prompt engineering is the practice of crafting..."
```

### Prompt Template

**Location:** `src/prompts.py`

```python
CONTEXT_GROUNDED_PROMPT = """You are a helpful assistant. Answer the 
following question using ONLY the provided context. If the answer 
cannot be found in the context, explicitly say you don't have that 
information.

Context:
{context}

Question: {question}

Answer:"""
```

### Key Concept: Grounding

**What is it?**
Ensuring responses are based on retrieved documents, not LLM's general knowledge.

**Benefits:**
- âœ… Reduces hallucinations (false information)
- âœ… Ensures accuracy
- âœ… Allows source attribution
- âœ… Domain-specific answers

**Implementation:**
```python
# Low temperature for grounded responses
effective_temperature = min(temperature, 0.2)

llm = ChatOpenAI(
    model=llm_model,
    temperature=effective_temperature,  # Cap at 0.2 for grounding
    max_tokens=max_tokens
)
```

### Temperature Control

**Temperature Parameter:**
Controls randomness/creativity of LLM output

| Temperature | Behavior | Use Case |
|-------------|----------|----------|
| 0.0 | Deterministic, always same answer | Factual Q&A |
| 0.2 | Low randomness, mostly factual | RAG/Grounded responses |
| 0.7 | Balanced | General chat |
| 1.0 | High randomness | Creative writing |
| 2.0 | Maximum randomness | Brainstorming |

**In Your Project:**
```python
# Slider in Sidebar
temperature = st.slider("Temperature", 0.0, 2.0, 0.3, step=0.1)

# But RAG chain caps it at 0.2
effective_temperature = min(temperature, 0.2) if temperature > 0.2 else temperature
```

### Chain Components

**1. Retriever:**
```python
top_chunks = retrieve(query=question, k=n_retrieve, db_path=db_path)
```

**2. Prompt Formatter:**
```python
prompt = ChatPromptTemplate.from_template(prompt_text)
formatted_prompt = prompt.invoke({
    "context": retrieved_context,
    "question": question
})
```

**3. LLM:**
```python
llm = ChatOpenAI(model=llm_model, temperature=temperature)
response = llm.invoke(formatted_prompt)
```

---

## 7. ğŸ’¾ Memory

### What It Is

Memory stores conversation history, allowing the chatbot to remember previous messages and maintain context across multiple turns.

### In Your Project

**Location:** `src/memory.py`

**Main Class:**
```python
class ConversationMemory:
    def __init__(self, max_history=10, session_id=None):
        """Initialize conversation memory."""
    
    def add_message(self, role, content, metadata=None):
        """Add user or assistant message."""
    
    def get_history_context(self, num_messages=6):
        """Return formatted conversation history."""
    
    def get_session_stats(self):
        """Get conversation statistics."""
    
    def export_to_json(self, filepath=None):
        """Save conversation to file."""
```

### Problem Without Memory

```
CONVERSATION WITHOUT MEMORY:

User: "What is machine learning?"
Bot:  "Machine learning is a subset of AI that enables systems to learn..."
      [Response forgotten after this turn]

User: "Give me examples of it"
Bot:  "What would you like examples of?"  â† Bot doesn't know "it" = ML
      âŒ Lost context
```

### Solution With Memory

```
CONVERSATION WITH MEMORY:

User: "What is machine learning?"
Bot:  "Machine learning is a subset of AI..."
      [Stored in memory]

User: "Give me examples of it"
Memory Lookup: "User previously asked about machine learning"
Bot:  "Sure! Examples of ML include neural networks, decision trees..."
      âœ… Context maintained
```

### Memory Structure

```json
{
    "session_id": "20251214_150230",
    "session_start": "2025-12-14T15:02:30",
    "history": [
        {
            "role": "user",
            "content": "What is AI?",
            "timestamp": "2025-12-14T15:02:30",
            "metadata": {}
        },
        {
            "role": "assistant",
            "content": "AI is artificial intelligence...",
            "timestamp": "2025-12-14T15:02:35",
            "metadata": {
                "llm_model": "gpt-4o",
                "temperature": 0.2
            }
        }
    ]
}
```

### Key Methods

**Adding Messages:**
```python
# Add user message
st.session_state.memory.add_message("user", question)

# Add assistant response
st.session_state.memory.add_message(
    "assistant",
    answer_text,
    metadata={
        "llm_model": llm_backend,
        "temperature": temperature,
        "pii_detected": bool(pii_in_response)
    }
)
```

**Getting Context:**
```python
# Get formatted history for LLM
memory_context = st.session_state.memory.get_history_context(num_messages=6)
# Returns: "User: What is AI?\nAssistant: AI is..."
```

**Statistics:**
```python
stats = st.session_state.memory.get_session_stats()
# Returns: {
#     "session_id": "20251214_150230",
#     "start_time": "2025-12-14T15:02:30",
#     "duration_seconds": 3600,
#     "total_messages": 24,
#     "user_messages": 12,
#     "assistant_messages": 12
# }
```

**Export:**
```python
filepath = st.session_state.memory.export_to_json()
# Saves to: ./logs/conversation_20251214_150230.json
```

### Integration in Your Chat

```python
# Initialize in main()
if "memory" not in st.session_state:
    st.session_state.memory = ConversationMemory(max_history=10)

# When user asks question
if question:
    # Add to memory
    st.session_state.memory.add_message("user", question)
    
    # Get context for RAG
    memory_context = st.session_state.memory.get_history_context()
    
    # Pass to RAG chain
    response = chain.invoke({
        "context": memory_context,
        "question": question
    })
    
    # Store response
    st.session_state.memory.add_message("assistant", response)
```

### Memory Manager (Optional)

For multi-session management:

```python
manager = MemoryManager(storage_dir="./data/memories")

# Create sessions
session1 = manager.create_session()
session2 = manager.create_session()

# Save all sessions
manager.export_all_sessions()
```

---

## Complete RAG Flow

### Full System Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INGESTION PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

[1] User uploads document via UI
    â””â”€ Supports: PDF, DOCX, TXT, Images, ZIP
    
[2] Document Loader processes file
    â””â”€ Extracts text, images, metadata
    
[3] Text Splitter chunks document
    â””â”€ Size: 500 chars, Overlap: 50 chars
    
[4] Generate Embeddings
    â””â”€ Model: text-embedding-3-small (384-dim)
    â””â”€ Or: text-embedding-3-large (1536-dim)
    
[5] Store in Vector Database
    â””â”€ ChromaDB: ./data/chroma_db
    â””â”€ Saves embeddings + metadata
    
[6] Store Embedding Config
    â””â”€ .embedding_config.json tracks model used

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QUERY/RESPONSE PIPELINE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

[1] User asks question via chat input
    â””â”€ Example: "What is machine learning?"
    
[2] Guardrail Checks
    â”œâ”€ Validate: Check length, banned keywords
    â”œâ”€ PII Check: Detect emails, SSN, credit cards
    â””â”€ Moderation: Content safety check
    
[3] Add to Conversation Memory
    â””â”€ Store user message with timestamp
    
[4] Retrieve Relevant Chunks
    â”œâ”€ Convert query to vector
    â”œâ”€ Search vector store
    â””â”€ Return top-3 most similar chunks
    
[5] Get Memory Context
    â””â”€ Retrieve last 6 messages from history
    
[6] Format Prompt
    â”œâ”€ System message: "You are a helpful assistant..."
    â”œâ”€ Context: Retrieved chunks
    â”œâ”€ Memory: Conversation history
    â””â”€ Question: User's query
    
[7] Create RAG Chain
    â””â”€ LangChain orchestrates the flow
    
[8] Call LLM
    â”œâ”€ Model: gpt-4o (or gpt-3.5-turbo)
    â”œâ”€ Temperature: 0.2 (capped for grounding)
    â””â”€ Max Tokens: 256
    
[9] Guardrail Checks on Response
    â””â”€ PII Redaction: Remove sensitive info
    
[10] Add to Memory
    â””â”€ Store assistant response with metadata
    
[11] Display to User
    â””â”€ Show in Streamlit chat interface
    
[12] Log to Audit Trail
    â””â”€ Track event: query_accepted, response_generated

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example Scenario

**Scenario:** User uploads "AI_Guide.pdf" and asks about machine learning

**Ingestion:**
```
Input: AI_Guide.pdf (20 pages, 50KB)
  â†’ Document Loader: Extract 20 documents (1 per page)
  â†’ Text Splitter: Create 150 chunks (500 chars each)
  â†’ Embeddings: Generate 150 embeddings (384 dimensions)
  â†’ Vector Store: Save to ChromaDB with metadata
  â†’ Config: Save "text-embedding-3-small" to config
```

**Query:**
```
User: "What is machine learning?"
  â†’ Guardrails: âœ… Pass (no PII, no banned content, safe)
  â†’ Memory: Add "user: What is machine learning?"
  â†’ Retriever: Convert query to vector â†’ Find top-3 similar chunks
    â”œâ”€ Chunk 15: "Machine learning is a branch of AI..." (0.92 similarity)
    â”œâ”€ Chunk 42: "Types of ML: supervised, unsupervised..." (0.88 similarity)
    â””â”€ Chunk 67: "Deep learning uses neural networks..." (0.85 similarity)
  â†’ Memory Context: "Previous 6 messages from conversation history"
  â†’ Prompt: Combine context + memory + question
  â†’ LLM (gpt-4o, temp=0.2): Generate answer
  â†’ Response: "Machine learning is a subset of AI where systems learn..."
  â†’ Guardrails: Redact any PII in response
  â†’ Memory: Add "assistant: Machine learning is..."
  â†’ Display: Show response in chat UI
  â†’ Audit: Log query_accepted and response_generated
```

---

## Configuration Reference

### Sidebar Configuration

| Parameter | Type | Range | Default | Impact |
|-----------|------|-------|---------|--------|
| Database Path | Text | - | ./data/chroma_db | Where vectors are stored |
| Embedding Model | Dropdown | small/large | small | Speed vs accuracy trade-off |
| LLM Model | Dropdown | gpt-4o/3.5 | gpt-4o | Intelligence level |
| Documents to Retrieve | Slider | 1-10 | 3 | How many chunks for context |
| Temperature | Slider | 0.0-2.0 | 0.3 | LLM creativity (capped at 0.2 for RAG) |
| Max Tokens | Slider | 50-1024 | 256 | Response length limit |

### File Locations

| Component | Location | Purpose |
|-----------|----------|---------|
| Documents | ./data/chroma_db/ | Vector database storage |
| Config | ./data/chroma_db/.embedding_config.json | Tracks embedding model used |
| Logs | ./logs/ | Application and guardrail logs |
| Audit Trail | ./audit.log | Security and compliance logging |
| Memories | ./data/memories/ | Exported conversation histories |

### Environment Variables

```
OPENAI_API_KEY=sk-proj-...          # OpenAI API key (from .env)
```

### Database Size Estimation

For reference, storage requirements:

- **Per Chunk:** ~1.5-2.5 KB (vector + metadata)
- **100 chunks:** ~200 KB
- **1000 chunks:** ~2 MB
- **10000 chunks:** ~20 MB

---

## Why RAG Architecture?

### Comparison Table

| Aspect | Traditional LLM | RAG System |
|--------|-----------------|-----------|
| **Accuracy** | âŒ Generic knowledge | âœ… Document-specific |
| **Freshness** | âŒ Outdated (trained data) | âœ… Current (your documents) |
| **Hallucinations** | âŒ Makes up information | âœ… Grounded in sources |
| **Citations** | âŒ No sources | âœ… Can cite chunks |
| **Domain Knowledge** | âŒ General purpose | âœ… Custom domain |
| **Privacy** | âŒ Data sent to API | âš ï¸ Vectors sent to API (mitigated) |
| **Cost** | âŒ Per token | âœ… Efficient (smaller context) |

### Real-World Benefits

1. **Enterprise Data:** Securely use proprietary documents
2. **Compliance:** Audit trails and PII redaction
3. **Accuracy:** Responses grounded in facts
4. **Freshness:** Updated documents reflect current info
5. **Explainability:** Can show which chunks informed response
6. **Cost:** Smaller context windows = lower API costs

---

## Security Features

Your system includes multiple security layers:

### Input Security
```
User Query
  â†’ Length validation (max 1500 chars)
  â†’ Banned keyword check
  â†’ PII detection (emails, SSN, credit cards, phones)
  â†’ Content moderation (violence, hate speech, etc.)
  â†’ Only proceeds if all checks pass
```

### Output Security
```
LLM Response
  â†’ PII redaction (automatic masking)
  â†’ Safety checks on generated content
  â†’ Audit logging (with PII redaction)
  â†’ Only displays safe, redacted content
```

### Audit Trail
```
./audit.log captures:
  - query_accepted
  - query_rejected (with reason)
  - pii_detected_in_query
  - pii_detected_in_response
  - response_generated
  - response_generation_error
  
All with automatic PII redaction
```

---

## Best Practices

### Document Preparation
1. **Quality:** Clean, well-structured documents
2. **Chunking:** Adjust chunk size based on content
3. **Metadata:** Meaningful source names and categories
4. **Updates:** Regularly ingest new/updated documents

### Query Optimization
1. **Clarity:** Ask specific questions
2. **Context:** Provide context if needed
3. **Follow-ups:** Use memory for related questions
4. **Refinement:** Adjust top-k if needed

### System Maintenance
1. **Monitor:** Check logs regularly
2. **Clean:** Remove outdated documents periodically
3. **Optimize:** Profile slow queries
4. **Backup:** Export important conversations

---

## Troubleshooting

### Common Issues

**Issue:** "No relevant context retrieved"
- **Cause:** Query doesn't match document content
- **Solution:** Upload documents matching your query domain

**Issue:** "Response seems generic"
- **Cause:** Retrieved chunks not specific enough
- **Solution:** Increase chunk_size or adjust document structure

**Issue:** "Slow retrieval"
- **Cause:** Large vector store or low-spec hardware
- **Solution:** Use text-embedding-3-small, limit chunk count

**Issue:** "Memory not persisting**
- **Cause:** Streamlit session resets between runs
- **Solution:** Export conversation regularly using export button

---

## Summary

Your RAG system represents a production-ready architecture that:

1. **Loads** documents from multiple formats
2. **Chunks** them intelligently
3. **Embeds** semantically using state-of-the-art models
4. **Stores** efficiently in a vector database
5. **Retrieves** relevant context via similarity search
6. **Chains** everything together with LLMs
7. **Remembers** conversation history for continuity
8. **Secures** with guardrails and audit logging

Every component serves a specific purpose in delivering accurate, grounded, and safe AI responses.

---

## Document Information

- **Project:** Capstone - Enterprise RAG Assistant
- **Version:** 1.0
- **Date:** December 14, 2025
- **Technology Stack:** Python, Streamlit, LangChain, OpenAI, ChromaDB
- **Architecture:** Retrieval-Augmented Generation (RAG)

---

*End of Document*
